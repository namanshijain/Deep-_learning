{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3YUSn4XM4oC"
      },
      "outputs": [],
      "source": [
        "# 1. Imports\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "def build_embedding_model(input_shape):\n",
        "\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(128)(x)\n",
        "\n",
        "    # L2 Normalization using a Lambda layer\n",
        "    outputs = layers.Lambda(lambda y: tf.nn.l2_normalize(y, axis=1))(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "CtZdiK4HNA8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Triplet Loss (Hard Mining)\n",
        "\n",
        "\n",
        "def triplet_loss(labels, embeddings, margin=0.2):\n",
        "\n",
        "    # Pairwise distance\n",
        "    dot_product = tf.matmul(embeddings, embeddings, transpose_b=True)\n",
        "    square_norm = tf.linalg.diag_part(dot_product)\n",
        "\n",
        "    distances = (\n",
        "        tf.expand_dims(square_norm, 1)\n",
        "        - 2.0 * dot_product\n",
        "        + tf.expand_dims(square_norm, 0)\n",
        "    )\n",
        "\n",
        "    distances = tf.maximum(distances, 0.0)\n",
        "\n",
        "    # Create masks\n",
        "    labels = tf.reshape(labels, (-1, 1))\n",
        "    positive_mask = tf.equal(labels, tf.transpose(labels))\n",
        "\n",
        "    # Remove self-comparisons\n",
        "    positive_mask = tf.logical_and(\n",
        "        positive_mask,\n",
        "        tf.logical_not(tf.eye(tf.shape(labels)[0], dtype=tf.bool))\n",
        "    )\n",
        "\n",
        "    negative_mask = tf.logical_not(positive_mask)\n",
        "\n",
        "    # Hardest positive\n",
        "    hardest_positive = tf.reduce_max(\n",
        "        tf.where(positive_mask, distances, tf.zeros_like(distances)),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Hardest negative\n",
        "    max_dist = tf.reduce_max(distances)\n",
        "    hardest_negative = tf.reduce_min(\n",
        "        tf.where(negative_mask, distances, max_dist),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Final loss\n",
        "    loss = tf.maximum(hardest_positive - hardest_negative + margin, 0.0)\n",
        "\n",
        "    return tf.reduce_mean(loss)\n"
      ],
      "metadata": {
        "id": "bVbxebQVNA-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Custom Training Model\n",
        "\n",
        "\n",
        "class TripletModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, embedding_model):\n",
        "        super().__init__()\n",
        "        self.embedding_model = embedding_model\n",
        "\n",
        "    def train_step(self, data):\n",
        "\n",
        "        images, labels = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            embeddings = self.embedding_model(images, training=True)\n",
        "            loss = triplet_loss(labels, embeddings)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.embedding_model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(gradients, self.embedding_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        return {\"loss\": loss}\n"
      ],
      "metadata": {
        "id": "qDE29AezNBA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Images → Embedding Model → Triplet Loss → Gradient → Update Embedding Model\n"
      ],
      "metadata": {
        "id": "UP8BORL_UhPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Load MNIST\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "y_train = y_train.astype(\"int32\")\n",
        "y_test = y_test.astype(\"int32\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aakJr6NANBDY",
        "outputId": "973f8b57-2b81-41c7-bba1-0998e45a9738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Create Dataset\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(10000).batch(64)\n"
      ],
      "metadata": {
        "id": "5eQuQXYlNBFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train Model\n",
        "\n",
        "\n",
        "embedding_model = build_embedding_model((28, 28, 1))\n",
        "\n",
        "model = TripletModel(embedding_model)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "\n",
        "model.fit(train_dataset, epochs=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut_wU1A_NBIR",
        "outputId": "57afadb6-4727-4408-bf80-d38119e79c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.2009\n",
            "Epoch 2/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 3/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 4/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 5/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 6/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 7/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 8/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 9/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 10/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 11/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 12/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 13/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 14/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 15/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 16/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 17/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 18/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 19/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n",
            "Epoch 20/20\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1998\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c96296ce750>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Test Similarity\n",
        "\n",
        "\n",
        "img1 = x_test[0:1]\n",
        "img2 = x_test[1:2]\n",
        "\n",
        "emb1 = embedding_model(img1)\n",
        "emb2 = embedding_model(img2)\n",
        "\n",
        "distance = tf.norm(emb1 - emb2)\n",
        "\n",
        "print(\"Distance:\", distance.numpy())\n",
        "\n",
        "\n",
        "threshold = 0.8\n",
        "\n",
        "if distance < threshold:\n",
        "    print(\"Images are SIMILAR\")\n",
        "else:\n",
        "    print(\"Images are DIFFERENT\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5VL5naENBLy",
        "outputId": "09318247-9370-49db-b430-668f288803cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance: 0.00069402286\n",
            "Images are SIMILAR\n"
          ]
        }
      ]
    }
  ]
}